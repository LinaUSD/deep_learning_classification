# -*- coding: utf-8 -*-
"""classification2.ipynb

Automatically generated by Colab.

#           Classification model Mnist dataset using  using ADAM optimizer/learner, and disply Confusion matrix 
"""

# import tools
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras import utils
from sklearn.metrics import confusion_matrix
import seaborn as sns

# import data
from keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

num_classes = 10

# encode labels
# the labels are integer numbers. we need to encod them using one hot encoding method-----> we will get labels in binary
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
y_train.shape
y_test.shape
y_train[0]
y_test[0]


# Data Preprocessing
# normalize the images ------ the pixel value range 0-255, convert it to 0-1
X_train = X_train/255.0
X_test = X_test/255.0



# flatten the train  and test input data. the DL model is just a dens NN or Fully connected NN. the dimension of Dens layer is just 1D can not process 2D data
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)


#Building the model
# Dens NN . first layer has 128 nodes and activation function is ReLU. Second Layer is Dens layer has  128 nodes and activation function is ReLU. last layer is a classification layer has 10 classes (activation function is softmax)
# Drop out is a technique used in NN to drop randamly number of nodes in the training process to avoid overfitting
model = Sequential()

model.add(Dense(units=128, input_shape=(784, ), activation="relu"))
model.add(Dense(units=128, activation="relu"))
model.add(Dropout(0.25))
model.add(Dense(units=10, activation="softmax"))

from keras.optimizers import Adam
optimizer = Adam(learning_rate=0.005)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()



# lets train the model: Batch size 512, # epochs 20
history = model.fit(X_train, y_train , epochs=20, batch_size=512,
                     validation_data=(X_test, y_test))

#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predict labels on test data
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1) # pick the highest probability with "np.argmax()", and turn it into an index uing "axis=1"
# save true label of this sample in a variable
y_true = np.argmax(y_test, axis=1)

y_pred [1340]

y_pred_classes [1340]

from sklearn.metrics import confusion_matrix
import seaborn as sns

plt.savefig('loss_curve_LR', format='jpeg')
plt.close()  # Close the plot to free up memory

# Evaluate the Classifier using Confusion Matrix
confusion_mtx = confusion_matrix(y_true, y_pred_classes)

# visualize confusion matrix with matplotlib and seaborn
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(confusion_mtx, annot=True, fmt="d", ax=ax, cmap="viridis")
ax.set_xlabel("Predicted Label")
ax.set_ylabel("True Label")
ax.set_title("Confusion Matrix");
